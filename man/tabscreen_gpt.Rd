% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tabscreen_chatgpt.R
\name{tabscreen_gpt}
\alias{tabscreen_gpt}
\title{Title and abstract screening with ChatGPT using function calls.}
\usage{
tabscreen_gpt(
  data,
  prompt,
  studyid,
  title,
  abstract,
  ...,
  arrange_var = studyid,
  model = "gpt-3.5-turbo-0613",
  role = "user",
  functions = incl_function_simple,
  function_call_name = list(name = "inclusion_decision_simple"),
  top_p = 1,
  time_info = TRUE,
  token_info = TRUE,
  api_key = get_api_key(),
  max_tries = 8,
  max_seconds = NULL,
  is_transient = gpt_is_transient,
  backoff = NULL,
  after = NULL,
  rpm = 3500,
  reps = 1,
  seed = NULL,
  progress = TRUE,
  messages = TRUE,
  incl_cutoff_upper = 0.5,
  incl_cutoff_lower = 0.4
)
}
\arguments{
\item{data}{Data with title and abstracts}

\item{prompt}{Prompt(s) to paste before title and abstract}

\item{studyid}{ID indication unique ID of study. Is missing, this is generated
automatically.}

\item{title}{Name of variable with containing title information}

\item{abstract}{Name of variable with containing abstract information}

\item{...}{Further argument to pass to the request body.
See \url{https://platform.openai.com/docs/api-reference/chat/create}.}

\item{arrange_var}{Function indicating the variables determining the arrangement
of the data. Default is \code{studyid}.}

\item{model}{Character string with the name of the completion model. Can take
multiple models, including gpt-4 models.Default = \code{"gpt-3.5-turbo-0613"}.
Find available model at
\url{https://platform.openai.com/docs/models/model-endpoint-compatibility}.}

\item{role}{Character string indicate the role of the user. Default is \code{"user"}.}

\item{functions}{Function to steer output. Default is \code{incl_function_simple}.
Find further documentation for function calling at
\url{https://openai.com/blog/function-calling-and-other-api-updates}.}

\item{function_call_name}{Functions to call.
Default is \code{list(name = "inclusion_decision_simple")}}

\item{top_p}{'An alternative to sampling with temperature, called nucleus sampling,
where the model considers the results of the tokens with top_p probability mass.
So 0.1 means only the tokens comprising the top 10\% probability mass are considered.
We generally recommend altering this or temperature but not both.' (OPEN-AI).
Find documentation at
\url{https://platform.openai.com/docs/api-reference/chat/create#chat/create-top_p}.}

\item{time_info}{Logical indicating whether the run time of each
request/question should be included in the data. Default = \code{TRUE}.}

\item{token_info}{Logical indicating whether the total number of tokens
per request should be included in the output data. Default = \code{TRUE}.}

\item{api_key}{Numerical value with your personal API key. Find at
\url{https://platform.openai.com/account/api-keys}. Use
\code{\link[=secret_make_key]{secret_make_key()}}, \code{\link[=secret_encrypt]{secret_encrypt()}}, and
\code{\link[=secret_decrypt]{secret_decrypt()}} to scramble and decrypt the api key and
use \code{\link[=set_api_key]{set_api_key()}} to securely automate the use of the
api key by setting the api key as a locale environment variable.}

\item{max_tries, max_seconds}{'Cap the maximum number of attempts with
\code{max_tries} or the total elapsed time from the first request with
\code{max_seconds}. If neither option is supplied (the default), \code{\link[=req_perform]{req_perform()}}
will not retry' (Wickham, 2023).}

\item{is_transient}{'A predicate function that takes a single argument
(the response) and returns \code{TRUE} or \code{FALSE} specifying whether or not
the response represents a transient error' (Wickham, 2023).}

\item{backoff}{'A function that takes a single argument (the number of failed
attempts so far) and returns the number of seconds to wait' (Wickham, 2023).}

\item{after}{'A function that takes a single argument (the response) and
returns either a number of seconds to wait or \code{NULL}, which indicates
that a precise wait time is not available that the \code{backoff} strategy
should be used instead' (Wickham, 2023).}

\item{rpm}{Numerical value indicating the number of requests per minute (rpm)
available for the specified api key. Find more information at
\url{https://platform.openai.com/docs/guides/rate-limits/what-are-the-rate-limits-for-our-api}.
Alternatively, use \code{\link[=rate_limits_per_minute]{rate_limits_per_minute()}}.}

\item{reps}{Numerical value indicating the number of times the same
question should be sent to ChatGPT. This can be useful to test consistency
between answers. Default is \code{1}.}

\item{seed}{Numerical value for a seed to ensure that proper,
parallel-safe random numbers are produced.}

\item{progress}{Logical indicating whether a progress line should be shown when running
the title and abstract screening in parallel. Default is \code{TRUE}.}

\item{messages}{Logical indicating whether to print messages. Defualt is \code{TRUE}.}

\item{incl_cutoff_upper}{Numerical value indicating the probability threshold
for which a studies should be included. Default is 0.5, which indicates that
titles and abstracts that ChatGPT has included more than 50 percent of the times
should be included.}

\item{incl_cutoff_lower}{Numerical value indicating the probability threshold
above which studies should be check by a human. Default is 0.4, which means
that if you ask ChatGPT the same questions 10 times and it includes the
title and abstract 4 times, we suggest that the study should be check by a human.}
}
\value{
A \code{tibble} with the gpt decision, run time, tokens used, top_p, and number
of repeated requests.
}
\description{
\ifelse{html}{\href{https://lifecycle.r-lib.org/articles/stages.html#stable}{\figure{lifecycle-stable.svg}{options: alt='[Stable]'}}}{\strong{[Stable]}}\if{html}{\out{<br>}}
\if{html}{\out{<br>}}
This function supports the conduct of title and abstract screening with ChatGPT in R.
The function allows to run title and abstract screening across multiple prompts and with
repeated questions to check for consistency across answers. This function draws
on the newly developed function calling to better steer output of response.
}
\examples{
\dontrun{
tabscreen_gpt(
  data = FFT_dat[1:2,],
  prompt = prompt,
  studyid = studyid,
  title = title,
  abstract = abstract,
  model = c("gpt-3.5-turbo-0613", "gpt-3.5-turbo-16k-0613"),
  max_tries = 1,
  reps = 10
  )
 }
}
\references{
Wickham H (2023).
\emph{httr2: Perform HTTP Requests and Process the Responses}.
https://httr2.r-lib.org, https://github.com/r-lib/httr2.
}
