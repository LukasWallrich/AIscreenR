---
title: "Using ChatGPT for Title and Abstract Screening in Systematic Reviews"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using ChatGPT for Title and Abstract Screening in Systematic Reviews}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  out.width = "100%"
)
```

<div class="warning" style='margin-left:2em; margin-right:2em; margin-bottom:2em; margin-top:2em; padding:0.1em; background-color: #d7dbdd; border: solid #bdc3c7 3px'>
<span>
<p style='margin-top:1em; text-align:center'>
<b>Important note</b></p>
<p style='margin:1em'>
This vignette represent tentative work-in-progress only. The general applicability and efficacy of using large-language models (LLMs) including ChatGPT for title and abstract screening in literature reviews are still unknown. Our first results suggest that ChatGPT can be a reliable second screener when reviewing specific and well defined interventions. The first evidence of this is shown in this vignette. However, be aware that our first results are based on a retrospective analysis which might not generalize to prospective screenings where ChatGPT potentially has not been trained on that reference data, that is data added to the internet after 2021. We have started testing the AIscreenR on more complex reviews. Here it seems to be harder to make concise prompts that can make ChatGPT emulating a reliable human second screener. Yet, the first tentative evidence for complex reviews suggests that ChatGPT can function well to reduce the total number of references that has to be screened by humans.
</p></span>
</div>

An all-important step to ensure the quality of systematic reviews involves detecting all relevant references related to the literature under review. Usually, this involves independent double screening of all references detected in relevant databases and literature with two human screeners. This procedure has shown pivotal since less experienced single screeners tend to miss around 13 % relavant studies (with 5 % for experienced screeners), which in most cases substantially change the main review findings (Waffenschmidt, 2019). Yet, double-screening is a costly and resource intensive procedure, excluding many researchers from using it. An alternative to human double-screening is to use automated tools to act as the second screener (Gartlehner et al., 2019; van de Schoot et al. 2021). Hereto, most evaluations of existing tools finds that most automated tools fail to reliably act as/imitating a human second screener. Meanwhile, it is still unknown how well or if the newly developed large-language models (LLMs) such as [ChatGPT](https://chat.openai.com/) can work and possibly emulate a human second screener. The aim of the `AIscreenR` package is to support the use and testing of ChatGPT as a the second screener or alternatively to reduce research waste. Concretely the package allow user to apply the gpt models from the [https://api.openai.com/v1/chat/completions](https://platform.openai.com/docs/models/model-endpoint-compatibility) endpoint. In future developments, we expect to add further LLMs (such as Bard) when APIs are available for those LLMs. For now, we invite other researchers to test this software so that we as a review community can get a better understanding of the performance of the gpt models from OpenAI as second screeners. In this vignette, we show how a title and abstract screening with ChatGPT can be done in R. The advantages of conducting the screening with ChatGPT via R is 1) that reviewers can easily work with a large number of references, avoiding copy-paste procedures, 2) that the total screening time can be substantially increased relative to using the ChatGPT interface, and 3) that consistency between gpt answer for the same title and abstract can easily be tested. In this vignette, we also show the first proof of concept for the use of ChatGPT as a reliable second screener. This goes without saying that ChatGPT works in all cases, and we think the tool should be use carefully and should always assisted by a human (human-in-the-loop). Consequently, we do not recommend to use the the ChatGPT as a single screener.   

## Loading relevant ris file data for screening

```{r setup, message=FALSE, warning=FALSE}
# Loading packages 
library(AIscreenR)
library(revtools)
library(tibble)
library(dplyr)
library(purrr)
library(usethis)
library(future)


# Loading excluded studies
# Consider making AIscreenR_example function as https://github.com/tidyverse/readxl/blob/HEAD/R/example.R 
excl_path <- system.file("extdata", "excl_tutorial.ris", package = "AIscreenR")
ris_dat_excl <- revtools::read_bibliography(excl_path) |> 
  suppressWarnings() |> 
  as_tibble() |>
  select(author, eppi_id, title, abstract) |> # Using only relevant variables
  mutate(
    human_code = 0
  )

# Loading included studies
incl_path <- system.file("extdata", "incl_tutorial.ris", package = "AIscreenR")
ris_dat_incl <- revtools::read_bibliography(incl_path) |> 
  suppressWarnings() |> 
  as_tibble() |>
  select(author, eppi_id, title, abstract) |>
  mutate(
    human_code = 1
  )

# Check why filges2015_data and filges2015_dat differs 
filges2015_dat <- 
  bind_rows(ris_dat_excl, ris_dat_incl) |> 
  mutate(
    studyid = 1:n()
  ) |> 
  relocate(studyid, .after = eppi_id)

#?filges2015_dat
filges2015_dat

```


## Getting API key OpenAI

Before you can use the function from `AIscreenR`, you must generate your own secret
API key. To do so you must first ensure that you have created an account at OpenAI (*if you have not done so at this stage, you can sign up [here](https://auth0.openai.com/u/login/identifier?state=hKFo2SBqQjNHSlc1ejIyREpUb01hdDF2OHEzQy12NnJwWlFUN6Fur3VuaXZlcnNhbC1sb2dpbqN0aWTZIEJSOWJaamdKLWswNGlfWDQ2NER1OXJmVUNpVmVzVjZfo2NpZNkgRFJpdnNubTJNdTQyVDNLT3BxZHR3QjNOWXZpSFl6d0Q)).* When having an account, go to [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys) and press the `+ Create new secret key` button (see picture below) and give your key a name. 

\  

```{r eval=TRUE, echo=FALSE}
knitr::include_graphics("helper-stuff/API_key_pic.png")
```
\ 

When you have generate your secret API key, remember to store it safely since
you will not be able see it again. **NOTE**: *If you lose your API key, you can 
just generate a new one*. 

## Handling your API key with `AIscreenR`

### Temporary solution

### Pemanent solution

To add your API key permanently as an environment variables, you can execute
`usethis::edit_r_environ()`.

\  

```{r eval=TRUE, echo=FALSE}
knitr::include_graphics("helper-stuff/Renviron.png")
```
\ 

After entering the API key, save the `.Renviron` document and restart `RStudio` (ctrl + shift + F10).

## Retrieve rate limit information 

```{r, eval=FALSE}
# Rate limits across one model (Default is "gpt-3.5-turbo-0613")
rate_limits <- rate_limits_per_minute()
rate_limits
#> # A tibble: 1 × 3
#>   model              requests_per_minute tokens_per_minute
#>   <chr>                            <dbl>             <dbl>
#> 1 gpt-3.5-turbo-0613                3500             90000


# Rate limits overview across multiple models
models <- c("gpt-3.5-turbo-0613", "gpt-4")
models_rate_limits <- rate_limits_per_minute(model = models) # Add further models if necessary
models_rate_limits
#> # A tibble: 2 × 3
#>   model              requests_per_minute tokens_per_minute
#>   <chr>                            <dbl>             <dbl>
#> 1 gpt-3.5-turbo-0613                3500             90000
#> 2 gpt-4                              200             10000
```

## Approximate price of screening

Example of how to enter a prompt. Can also be done in word.

```{r}
prompt <- "Evaluate the following study based on the selection criteria
for a systematic review on the effects of family-based interventions on drug 
abuse reduction for young people in treatment for non-opioid drug use.
A family-based intervention (FFT) is equivalent to a behavior focused
family therapy, where young people’s drug use is understood in relation to 
family behavior problems. Family-based interventions also includes manual-based 
family therapies as it targets young people and their families as a system 
throughout treatment, and thereby recognizes the important role of the family 
system in the development and treatment of young people’s drug use problems. 
FFT was developed in the late 1980s on request from the US National Institute on
Drug Abuse (NIDA). The development of FFT was initially heavily inspired by the 
alcohol abuse program Community Reinforcement Approach (CRA), which was aimed 
at restructuring the environment to reinforce non-alcohol associated activities. 
FFT developed to have more emphasis on contingency contracting, impulse control 
strategies specific to drug use, and increased emphasis on involvement of family 
members in treatment. FFT is designed to accommodate diverse populations of 
youths with a variety of behavioral, cultural and individual preferences. 
FFT has evolved for use in severe behavioral disturbances known to co-exist with
substance use and dependence, and the core interventions have been enhanced to 
address several mental health related problems commonly occurring
as comorbid conditions in drug use treatment participant.  For each study,
I would like you to assess:  1) Is the study about a family-based intervention,
such as Functional Family Therapy, Multidimensional Family Therapy, or
Behavioral Family Therapy? (Outpatient manual-based interventions of any
duration delivered to young people and their families). If not, exclude study.
2) Are the participants in outpatient drug treatment primarily
for non-opioid drug use? 3) Are the participants within age 11–21?"
```

Approximate price of screening

```{r, echo = FALSE}
models <- c("gpt-3.5-turbo-0613", "gpt-4")
```

```{r}
reps <- c(10, 1)

app_obj <- 
  approximate_price_gpt(
    data = filges2015_dat, # Tutorial data embedded in the package
    prompt = prompt, 
    studyid = studyid, # indicate the variable with the studyid in the data
    title = title, # indicate the variable with the titles in the data
    abstract = abstract, # indicate the variable with the abstracts in the data
    model = models,
    reps = reps 
  )

app_obj

app_obj$price_dollar
app_obj$price_data

```

## Screen titles and abstracts
```{r, message=FALSE, eval = FALSE}
plan(multisession)

FFT_screen_obj <- 
  tabscreen_gpt(
    data = filges2015_dat[c(1:2),],
    prompt = prompt,
    studyid = studyid, # indicate the variable with the studyid in the data
    title = title, # indicate the variable with the titles in the data
    abstract = abstract, # indicate the variable with the abstracts in the data,
    model = models,
    reps = reps
  )
#> * The approximate price of the current (simple) screening will be around $0.0845.
#>  Progress: ────────────────────────────────────────────────────────────────────────── 100%

plan(sequential)

FFT_screen_obj
#> Find data with all answers by executing
#>  object_name$answer_data_all 
#> 
#> Find data with the result aggregated across multiple answers by executing
#>  object_name$answer_data_sum
#> 
#> Find total price for the screening by executing
#>  object_name$price_dollor
#>  
#> If some requests failed, find the error data by executing
#>  object_name$error_data
  
```


# References
Gartlehner, G., Wagner, G., Lux, L. et al. (2019). Assessing the accuracy of machine-assisted abstract screening with DistillerAI: a user study. *Systematic Reviews*. 8, 1-10. <https://doi.org/10.1186/s13643-019-1221-3>

Waffenschmidt, S., Knelangen, M., Sieben, W., Bühn, S., & Pieper, D. (2019). Single screening versus conventional double screening for study selection in systematic reviews: a methodological systematic review. *BMC Medical Research Methodology* 19, 1-9. <https://doi.org/10.1186/s12874-019-0782-0>

Westgate MJ (2019). revtools: An R package to support article screening for evidence synthesis. *Research Synthesis Methods*. doi:10.1002/jrsm.1374 <https://doi.org/10.1002/jrsm.1374>.
