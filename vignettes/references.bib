Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Syriani2024,
abstract = {Systematic reviews (SRs) provide valuable evidence for guiding new research directions. However, the manual effort involved in selecting articles for inclusion in an SR is error-prone and time-consuming. While screening articles has traditionally been considered challenging to automate, the advent of large language models offers new possibilities. In this paper, we discuss the effect of using ChatGPT on the SR process. In particular, we investigate the effectiveness of different prompt strategies for automating the article screening process using five real SR datasets. Our results show that ChatGPT can reach up to 82% accuracy. The best performing prompts specify exclusion criteria and avoid negative shots. However, prompts should be adapted to different corpus characteristics.},
author = {Syriani, Eugene and David, Istvan and Kumar, Gauransh},
doi = {https://doi.org/10.1016/j.cola.2024.101287},
issn = {2590-1184},
journal = {Journal of Computer Languages},
keywords = {Empirical research,GPT,Generative AI,Large language model,Literature review,Mapping study,Screening},
pages = {101287},
title = {{Screening articles for systematic reviews with ChatGPT}},
url = {https://www.sciencedirect.com/science/article/pii/S2590118424000303},
volume = {80},
year = {2024}
}
@article{westgate2019,
abstract = {The field of evidence synthesis is growing rapidly, with a corresponding increase in the number of software tools and workflows to support the construction of systematic reviews, systematic maps, and meta-analyses. Despite much progress, however, a number of problems remain, including slow integration of new statistical or methodological approaches into user-friendly software, low prevalence of open-source software, and poor integration among distinct software tools. These issues hinder the utility and transparency of new methods to the research community. Here, I present revtools, an R package to support article screening during evidence synthesis projects. It provides tools for the import and deduplication of bibliographic data, screening of articles by title or abstract, and visualization of article content using topic models. The software is entirely open-source and combines command-line scripting for experienced programmers with custom-built user interfaces for casual users, with further methods to support article screening to be added over time. revtools provides free access to novel methods in an open-source environment and represents a valuable step in expanding the capacity of R to support evidence synthesis projects.},
author = {Westgate, Martin J},
doi = {10.1002/jrsm.1374},
issn = {1759-2879},
journal = {Research Synthesis Methods},
keywords = {data visualization,meta-analysis,natural language processing,systematic review,topic models},
month = {dec},
number = {4},
pages = {606--614},
publisher = {John Wiley & Sons, Ltd},
title = {{revtools: An R package to support article screening for evidence synthesis}},
url = {https://doi.org/10.1002/jrsm.1374},
volume = {10},
year = {2019}
}
@article{Alshami2023,
author = {Alshami, Ahmad and Elsayed, Moustafa and Ali, Eslam and Eltoukhy, Abdelrahman E E and Zayed, Tarek},
doi = {10.3390/systems11070351},
issn = {2079-8954},
journal = {Systems},
keywords = {Alshami2024},
number = {7},
pages = {351},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Harnessing the power of ChatGPT for automating systematic review process: Methodology, case study, limitations, and future directions}},
volume = {11},
year = {2023}
}
@article{Vembye2024_gpt,
author = {Vembye, Mikkel H. and Christensen, Julian and M{\o}lgaard, Anja B. and Schytt, Frederikk L. W.},
doi = {10.31219/osf.io/yrhzm},
journal = {Open Science Framework},
title = {{GPT API Models Can Function as Highly Reliable Second Screeners of Titles and Abstracts in Systematic Reviews: A Proof of Concept and Common Guidelines}},
year = {2024}
}
@article{Gargari2024,
abstract = {After conducting a database search, the subsequent phase in the execution of systematic reviews (SRs) involves title and abstract screening.1 This stage bears significant importance and necessitates the involvement of dedicated and experienced researchers who can exhibit precision and accuracy, particularly when the search yields a substantial number of studies. Besides the qualities of experience and dedication demonstrated by the screeners, several other factors influence the quality of the screening process, such as effective team management, the adoption of a double-screening approach and, notably, the implementation of a well-structured screening design. A screening tool comprises a set of questions that must be addressed by the screeners, and these questions should adhere to the following criteria: (1) they must be objective, (2) they should be single-barrelled and (3) they should encompass questions answerable with ‘yes', ‘no' or ‘unsure' responses.2 The domain of large language and transformer models has showcased a promising trajectory of advancement, consistently improving day by day. These models are trained on a vast corpora of text and possess the capability to comprehend and generate human-like text.3 A prominent example within this realm is the Generative Pre-Trained Transformer (GPT) developed by OpenAI, with the latest iteration being GPT-4 at the time of composing this discourse. GPT-4 has exhibited commendable performance across a range of human-related tasks and has surpassed its predecessor, GPT-3.5, in evaluations conducted by the company.4 This single-case study was conceived to assess the performance of GPT 3.5 in the context of title and abstract screening for SRs. To execute this task, a recently published SR titled ‘Light Therapy in Insomnia Disorder: A Systematic Review and Meta-Analysis' was selected, and the databases were queried using the keywords stipulated in the original paper.5 Two key rationales underpinned the selection of {\ldots}},
author = {Gargari, Omid Kohandel and Mahmoudi, Mohammad Hossein and Hajisafarali, Mahsa and Samiee, Reza},
doi = {10.1136/bmjebm-2023-112678},
journal = {BMJ Evidence-Based Medicine},
month = {feb},
number = {1},
pages = {69 LP -- 70},
title = {{Enhancing title and abstract screening for systematic reviews with GPT-3.5 turbo}},
url = {http://ebm.bmj.com/content/29/1/69.abstract},
volume = {29},
year = {2024}
}
@article{Issaiy2024,
abstract = {The screening process for systematic reviews and meta-analyses in medical research is a labor-intensive and time-consuming task. While machine learning and deep learning have been applied to facilitate this process, these methods often require training data and user annotation. This study aims to assess the efficacy of ChatGPT, a large language model based on the Generative Pretrained Transformers (GPT) architecture, in automating the screening process for systematic reviews in radiology without the need for training data.},
author = {Issaiy, Mahbod and Ghanaati, Hossein and Kolahi, Shahriar and Shakiba, Madjid and Jalali, Amir Hossein and Zarei, Diana and Kazemian, Sina and Avanaki, Mahsa Alborzi and Firouznia, Kavous},
doi = {10.1186/s12874-024-02203-8},
issn = {1471-2288},
journal = {BMC Medical Research Methodology},
number = {1},
pages = {78},
title = {{Methodological insights into ChatGPT's screening performance in systematic reviews}},
url = {https://doi.org/10.1186/s12874-024-02203-8},
volume = {24},
year = {2024}
}
@article{Syriani2023,
author = {Syriani, Eugene and David, Istvan and Kumar, Gauransh},
journal = {arXiv preprint arXiv:2307.06464},
title = {{Assessing the ability of ChatGPT to screen articles for systematic reviews}},
year = {2023}
}
@article{Khraisha2024,
abstract = {Abstract Systematic reviews are vital for guiding practice, research and policy, although they are often slow and labour-intensive. Large language models (LLMs) could speed up and automate systematic reviews, but their performance in such tasks has yet to be comprehensively evaluated against humans, and no study has tested Generative Pre-Trained Transformer (GPT)-4, the biggest LLM so far. This pre-registered study uses a ?human-out-of-the-loop? approach to evaluate GPT-4's capability in title/abstract screening, full-text review and data extraction across various literature types and languages. Although GPT-4 had accuracy on par with human performance in some tasks, results were skewed by chance agreement and dataset imbalance. Adjusting for these caused performance scores to drop across all stages: for data extraction, performance was moderate, and for screening, it ranged from none in highly balanced literature datasets ($\sim$1:1) to moderate in those datasets where the ratio of inclusion to exclusion in studies was imbalanced ($\sim$1:3). When screening full-text literature using highly reliable prompts, GPT-4's performance was more robust, reaching ?human-like? levels. Although our findings indicate that, currently, substantial caution should be exercised if LLMs are being used to conduct systematic reviews, they also offer preliminary evidence that, for certain review tasks delivered under specific conditions, LLMs can rival human performance.},
author = {Khraisha, Qusai and Put, Sophie and Kappenberg, Johanna and Warraitch, Azza and Hadfield, Kristin},
doi = {10.1002/jrsm.1715},
issn = {1759-2879},
journal = {Research Synthesis Methods},
keywords = {GPT,artificial intelligence (AI),large language models (LLMs),machine learning,natural language processing (NLP),systematic reviews},
month = {mar},
publisher = {John Wiley & Sons, Ltd},
title = {{Can large language models replace humans in systematic reviews? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages}},
url = {https://doi.org/10.1002/jrsm.1715},
year = {2024}
}
@article{Guo2024,
abstract = {Background: The systematic review of clinical research papers is a labor-intensive and time-consuming process that often involves the screening of thousands of titles and abstracts. The accuracy and efficiency of this process are critical for the quality of the review and subsequent health care decisions. Traditional methods rely heavily on human reviewers, often requiring a significant investment of time and resources. Objective: This study aims to assess the performance of the OpenAI generative pretrained transformer (GPT) and GPT-4 application programming interfaces (APIs) in accurately and efficiently identifying relevant titles and abstracts from real-world clinical review data sets and comparing their performance against ground truth labeling by 2 independent human reviewers. Methods: We introduce a novel workflow using the Chat GPT and GPT-4 APIs for screening titles and abstracts in clinical reviews. A Python script was created to make calls to the API with the screening criteria in natural language and a corpus of title and abstract data sets filtered by a minimum of 2 human reviewers. We compared the performance of our model against human-reviewed papers across 6 review papers, screening over 24,000 titles and abstracts. Results: Our results show an accuracy of 0.91, a macro F1-score of 0.60, a sensitivity of excluded papers of 0.91, and a sensitivity of included papers of 0.76. The interrater variability between 2 independent human screeners was $\kappa$=0.46, and the prevalence and bias-adjusted $\kappa$ between our proposed methods and the consensus-based human decisions was $\kappa$=0.96. On a randomly selected subset of papers, the GPT models demonstrated the ability to provide reasoning for their decisions and corrected their initial decisions upon being asked to explain their reasoning for incorrect classifications. Conclusions: Large language models have the potential to streamline the clinical review process, save valuable time and effort for researchers, and contribute to the overall quality of clinical reviews. By prioritizing the workflow and acting as an aid rather than a replacement for researchers and reviewers, models such as GPT-4 can enhance efficiency and lead to more accurate and reliable conclusions in medical research.},
author = {Guo, Eddie and Gupta, Mehul and Deng, Jiawen and Park, Ye-Jean and Paget, Michael and Naugler, Christopher},
doi = {10.2196/48996},
issn = {1438-8871},
journal = {J Med Internet Res},
keywords = {Chat GPT,GPT,GPT-4,LLM,NLP,abstract screening,classification,extract,extraction,free text,language model,large language models,natural language processing,nonopiod analgesia,review methodology,review methods,screening,systematic,systematic review,unstructured data},
pages = {e48996},
title = {{Automated paper screening for clinical reviews using large language models: Data analysis study}},
url = {https://www.jmir.org/2024/1/e48996 https://doi.org/10.2196/48996 http://www.ncbi.nlm.nih.gov/pubmed/38214966},
volume = {26},
year = {2024}
}
